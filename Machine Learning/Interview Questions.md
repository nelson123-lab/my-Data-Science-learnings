WHat is Overfitting ?
- Overfitting is a common problem in machine learning where a model is trained too well on the training data, to the point that it starts to memorize the data         instead of learning the underlying patterns. This results in a model that performs well on the training data but poorly on new, unseen data.

What is Regulariztion and what are the different types ?
- Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function of the model. This penalty term discourages the model from   fitting the training data too closely and encourages it to learn the underlying patterns instead. There are several regularization techniques, but two of the most   popular ones are L1 regularization and L2 regularization.
- L1 regularization, also known as Lasso regularization, adds a penalty term to the loss function that is proportional to the absolute value of the model weights.     This results in a sparse model where some of the weights are set to zero, effectively removing some of the features from the model.
- L2 regularization, also known as Ridge regularization, adds a penalty term to the loss function that is proportional to the square of the model weights. This         results in a model where all the features are used, but the weights are smaller and more evenly distributed.
